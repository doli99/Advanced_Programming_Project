{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7865a96",
   "metadata": {},
   "source": [
    "# Large-Scale Data Partitioning For NYC Taxi Dataset 2021\n",
    "\n",
    "\n",
    "NYC open data portal provides large open source dataset of taxi trips for specific year for our analytics purposes we selected 2021 the dataset contained 30.8 million rows. In order to be able to load the dataset and work on it we used Dask to partition the dataset into managable parts and work on it accordingly.In this section we use Dask to help us split the large dataset into 4 different partitions in order to be able to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c72a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in /Users/md/opt/anaconda3/lib/python3.9/site-packages (2021.10.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from dask) (2.0.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from dask) (2021.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from dask) (21.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from dask) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from dask) (0.11.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/md/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->dask) (3.0.4)\n",
      "Requirement already satisfied: locket in /Users/md/opt/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg (from partd>=0.3.10->dask) (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# syntax used to install Dask \n",
    "pip install dask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f814dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports to run the code below\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77dea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data types for each column\n",
    "\n",
    "dtypes = {\n",
    "    'VendorID': 'float64',\n",
    "    'tpep_pickup_datetime': 'object',  # Assuming this is a string representing date & time\n",
    "    'tpep_dropoff_datetime': 'object', # Assuming this is a string representing date & time\n",
    "    'passenger_count': 'float64',\n",
    "    'trip_distance': 'float64',\n",
    "    'RatecodeID': 'float64',\n",
    "    'store_and_fwd_flag': 'object',\n",
    "    'PULocationID': 'float64',\n",
    "    'DOLocationID': 'float64',\n",
    "    'payment_type': 'float64',\n",
    "    'fare_amount': 'float64',\n",
    "    'extra': 'float64',\n",
    "    'mta_tax': 'float64',\n",
    "    'tip_amount': 'float64',\n",
    "    'tolls_amount': 'float64',\n",
    "    'improvement_surcharge': 'float64',\n",
    "    'total_amount': 'float64',\n",
    "    'congestion_surcharge': 'float64'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We run the code below only once to split the dataset into 4 managable partitions.\n",
    "# Read the CSV file with specified data types\n",
    "\n",
    "#df = dd.read_csv('2021_TLC_NYC.csv', dtype=dtypes)\n",
    "#desired_number_of_partitions = 4\n",
    "\n",
    "# Split the DataFrame into smaller partitions\n",
    "#df_split = df.repartition(npartitions=desired_number_of_partitions)\n",
    "\n",
    "# Iterate over partitions and save each partition to a separate CSV file\n",
    "#for i, partition in enumerate(df_split.to_delayed()):\n",
    "    #partition.compute().to_csv(f'2021_TLC_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c83fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with specified data types\n",
    "\n",
    "df1 = pd.read_csv(\"2021_TLC_0.csv\", dtype=dtypes)\n",
    "df2 = pd.read_csv(\"2021_TLC_1.csv\", dtype=dtypes)\n",
    "df3 = pd.read_csv(\"2021_TLC_2.csv\", dtype=dtypes)\n",
    "df4 = pd.read_csv(\"2021_TLC_3.csv\", dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757ae0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in df1: 7919804\n",
      "Number of records in df2: 7887665\n",
      "Number of records in df3: 7873284\n",
      "Number of records in df4: 7223319\n"
     ]
    }
   ],
   "source": [
    "#in order to make sure that during partitioning we did not lose data we can count rows in each dataset \n",
    "#and compare them to the original dataset\n",
    "\n",
    "count_df1 = df1.shape[0]\n",
    "count_df2 = df2.shape[0]\n",
    "count_df3 = df3.shape[0]\n",
    "count_df4 = df4.shape[0]\n",
    "\n",
    "print(\"Number of records in df1:\", count_df1)\n",
    "print(\"Number of records in df2:\", count_df2)\n",
    "print(\"Number of records in df3:\", count_df3)\n",
    "print(\"Number of records in df4:\", count_df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a857cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records across all dataframes: 30904072\n"
     ]
    }
   ],
   "source": [
    "#in order to see that we did not lose any data during the partitioning \n",
    "#we sum up the number of records for each csv file created\n",
    "total_records = count_df1 + count_df2 + count_df3 + count_df4\n",
    "print(\"Total number of records across all dataframes:\", total_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d099b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/md/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the original dataset: 30904072\n"
     ]
    }
   ],
   "source": [
    "#now letc scompare the number to that of original dataset:\n",
    "df_original = pd.read_csv(\"2021_TLC_NYC.csv\") \n",
    "count_original = df_original.shape[0]\n",
    "print(\"Number of records in the original dataset:\", count_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ad09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total records match the original dataset.\n"
     ]
    }
   ],
   "source": [
    "#Lets compare the numbers and see if they match using an if/else statement:\n",
    "if total_records == count_original:\n",
    "    print(\"The total records match the original dataset.\")\n",
    "else:\n",
    "    print(\"There is a discrepancy in the record counts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd374d0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This section of the Python code is dedicated to data preparation, focusing on a large dataset sourced from the NYC Open Data portal, detailing taxi trips from the year 2021. Given the dataset's size, which contains approximately 30.8 million rows, using a standard data processing tool like pandas would be inefficient due to memory constraints. To address this, the Dask library is employed for its ability to handle large datasets efficiently by partitioning them into smaller, more manageable segments. The code demonstrates how to read the large dataset using Dask, explicitly defining data types for each column to ensure data integrity and efficient memory usage. The dataset is then split into four partitions, and each partition is saved as a separate CSV file. Following this, the partitions are verified by comparing the total number of records in all partitions against the original dataset, ensuring that no data was lost during the partitioning process. This ensures data consistency and readiness for subsequent analysis phases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
